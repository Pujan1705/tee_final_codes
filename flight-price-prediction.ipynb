{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### This notebook and solution includes EDA, Visualisations, Feature engineering and the execution of 6 different ML Models:\n\n#### Linear Regression, Decision Trees, Random Forests, XGBoost, AdaBoost and Gradient Boost.\n\n#### Each model gave a prediction accuracy of over 90% with the Random Forest Regression having the highest accuracy of 96.5%\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-02-25T14:23:29.276922Z","iopub.execute_input":"2023-02-25T14:23:29.277668Z","iopub.status.idle":"2023-02-25T14:23:30.861685Z","shell.execute_reply.started":"2023-02-25T14:23:29.277618Z","shell.execute_reply":"2023-02-25T14:23:30.860484Z"}}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport os\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import MinMaxScaler","metadata":{"execution":{"iopub.status.busy":"2023-02-25T18:21:52.886496Z","iopub.execute_input":"2023-02-25T18:21:52.886917Z","iopub.status.idle":"2023-02-25T18:21:52.895552Z","shell.execute_reply.started":"2023-02-25T18:21:52.886883Z","shell.execute_reply":"2023-02-25T18:21:52.894439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing of data and EDA:","metadata":{}},{"cell_type":"code","source":"df= pd.read_csv('/kaggle/input/flight-price-prediction/Clean_Dataset.csv')","metadata":{"execution":{"iopub.status.busy":"2023-02-25T17:45:36.090489Z","iopub.execute_input":"2023-02-25T17:45:36.090851Z","iopub.status.idle":"2023-02-25T17:45:36.407012Z","shell.execute_reply.started":"2023-02-25T17:45:36.09082Z","shell.execute_reply":"2023-02-25T17:45:36.405982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2023-02-25T17:45:37.225054Z","iopub.execute_input":"2023-02-25T17:45:37.225756Z","iopub.status.idle":"2023-02-25T17:45:37.248741Z","shell.execute_reply.started":"2023-02-25T17:45:37.225718Z","shell.execute_reply":"2023-02-25T17:45:37.247483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=df.drop('Unnamed: 0', axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-02-25T17:45:40.164846Z","iopub.execute_input":"2023-02-25T17:45:40.165422Z","iopub.status.idle":"2023-02-25T17:45:40.192632Z","shell.execute_reply.started":"2023-02-25T17:45:40.165387Z","shell.execute_reply":"2023-02-25T17:45:40.191618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=df.drop('flight', axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-02-25T17:45:40.392805Z","iopub.execute_input":"2023-02-25T17:45:40.39389Z","iopub.status.idle":"2023-02-25T17:45:40.430185Z","shell.execute_reply.started":"2023-02-25T17:45:40.393842Z","shell.execute_reply":"2023-02-25T17:45:40.429159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2023-02-25T17:45:40.815377Z","iopub.execute_input":"2023-02-25T17:45:40.816343Z","iopub.status.idle":"2023-02-25T17:45:40.836253Z","shell.execute_reply.started":"2023-02-25T17:45:40.816291Z","shell.execute_reply":"2023-02-25T17:45:40.835139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2023-02-25T17:45:43.023996Z","iopub.execute_input":"2023-02-25T17:45:43.024386Z","iopub.status.idle":"2023-02-25T17:45:43.065567Z","shell.execute_reply.started":"2023-02-25T17:45:43.02435Z","shell.execute_reply":"2023-02-25T17:45:43.064428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['airline'].unique()","metadata":{"execution":{"iopub.status.busy":"2023-02-25T17:45:46.838568Z","iopub.execute_input":"2023-02-25T17:45:46.838957Z","iopub.status.idle":"2023-02-25T17:45:46.866564Z","shell.execute_reply.started":"2023-02-25T17:45:46.838923Z","shell.execute_reply":"2023-02-25T17:45:46.865417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualising Some Features:","metadata":{}},{"cell_type":"markdown","source":"#### 1. Number of passengers per airline","metadata":{}},{"cell_type":"code","source":"airline_counts = df['airline'].value_counts().sort_values(ascending=True)\nsns.set_style(\"whitegrid\")\ncolors = ['#4C72B0', '#55A868', '#C44E52', '#8172B2', '#CCB974', '#64B5CD']\n\n# Create horizontal bar chart of airline counts\nairline_counts.plot(kind='barh', color=colors)\nplt.title(\"Customer Count by Airline\")\nplt.xlabel(\"Count\")\nplt.ylabel(\"Airline\")\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-02-25T18:03:44.664003Z","iopub.execute_input":"2023-02-25T18:03:44.665062Z","iopub.status.idle":"2023-02-25T18:03:44.922582Z","shell.execute_reply.started":"2023-02-25T18:03:44.665021Z","shell.execute_reply":"2023-02-25T18:03:44.921545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2. Average ticket price for each airline","metadata":{}},{"cell_type":"code","source":"avg_price = df.groupby('airline')['price'].mean().reset_index()\navg_price = avg_price.sort_values(by='price',ascending=False)\nsns.barplot(x='airline', y='price', data=avg_price)\n\nplt.xlabel('Airline')\nplt.ylabel('Average Price')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-25T18:15:11.622266Z","iopub.execute_input":"2023-02-25T18:15:11.622944Z","iopub.status.idle":"2023-02-25T18:15:11.894237Z","shell.execute_reply.started":"2023-02-25T18:15:11.622909Z","shell.execute_reply":"2023-02-25T18:15:11.893163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3. Number of passengers in Business and Economy Class","metadata":{}},{"cell_type":"code","source":"class_counts = df['class'].value_counts()\ncolors = ['#FFD700', '#ed8e51']\nclass_counts.plot(kind='pie', colors=colors)\nplt.title(\"Number of fliers in Business Vs Economy Class:\")\nplt.ylabel('')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-02-25T17:53:53.693154Z","iopub.execute_input":"2023-02-25T17:53:53.69406Z","iopub.status.idle":"2023-02-25T17:53:53.812264Z","shell.execute_reply.started":"2023-02-25T17:53:53.69401Z","shell.execute_reply":"2023-02-25T17:53:53.810559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4. Ticket prices based on class","metadata":{}},{"cell_type":"code","source":"class_prices = df.groupby('class')['price'].mean()\nsns.set_style(\"whitegrid\")\nclass_prices.plot(kind='bar', color=['#4C72B0', '#55A868'])\nplt.title(\"Average Ticket Price by Airplane Class\")\nplt.xlabel(\"Class\")\nplt.ylabel(\"Price)\")\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-02-25T17:56:36.076109Z","iopub.execute_input":"2023-02-25T17:56:36.07675Z","iopub.status.idle":"2023-02-25T17:56:36.31865Z","shell.execute_reply.started":"2023-02-25T17:56:36.076701Z","shell.execute_reply":"2023-02-25T17:56:36.317639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 5. Ticket prices based on duration of flight","metadata":{}},{"cell_type":"code","source":"plt.scatter(df['duration'], df['price'], s=2, color= '#ed8e51')\n\nplt.title(\"Flight Duration vs Ticket Price\")\nplt.xlabel(\"Duration of Flight\")\nplt.ylabel(\"Ticket Price\")\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-02-25T18:08:03.583148Z","iopub.execute_input":"2023-02-25T18:08:03.584363Z","iopub.status.idle":"2023-02-25T18:08:04.217493Z","shell.execute_reply.started":"2023-02-25T18:08:03.584319Z","shell.execute_reply":"2023-02-25T18:08:04.21614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 6. Relation between number of stops for a flight and the flight ticket price","metadata":{}},{"cell_type":"code","source":"# Create box plot of number of stops vs ticket price\ndf.boxplot(column='price', by='stops')\n\nplt.title(\"\")\nplt.xlabel(\"Number of Stops\")\nplt.ylabel(\"Ticket Price\")\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-02-25T18:18:27.206508Z","iopub.execute_input":"2023-02-25T18:18:27.206892Z","iopub.status.idle":"2023-02-25T18:18:27.647278Z","shell.execute_reply.started":"2023-02-25T18:18:27.206859Z","shell.execute_reply":"2023-02-25T18:18:27.646199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  Identifying the categorical features:","metadata":{}},{"cell_type":"code","source":"# capturing those of type *object*\n\ncat_cols = list(df.select_dtypes(include=['object']).columns)\nprint(f\"Number of categorical columns: {len(cat_cols)}\")\nprint(f\"Categorical columns:\\n{cat_cols}\")","metadata":{"execution":{"iopub.status.busy":"2023-02-25T15:18:01.155254Z","iopub.execute_input":"2023-02-25T15:18:01.155737Z","iopub.status.idle":"2023-02-25T15:18:01.184467Z","shell.execute_reply.started":"2023-02-25T15:18:01.15569Z","shell.execute_reply":"2023-02-25T15:18:01.183288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Performing target encoding for all categorical variables:","metadata":{}},{"cell_type":"markdown","source":"* Target encoding, also known as likelihood encoding, is a method of encoding categorical features in which each category is replaced with the mean (or median) of the target variable for that category. In other words, we use the target variable to encode the categories of a categorical feature.\n\n*  For example, in our dataset we have a categorical feature called \"airline\" and a target variable called \"price\". We can calculate the mean price for each airline and use these means as the new values for the \"airline\" feature. This way, we are encoding the \"airline\" feature with information from the target variable \"price\".\n\n*  The advantage of target encoding is that it can capture the relationship between the categorical feature and the target variable in a more precise way than one-hot encoding, especially when the categorical feature has a large number of categories. Target encoding can also reduce the dimensionality of the feature space.\n\n* However, target encoding has a potential risk of overfitting if there are too few samples for some categories, leading to a high variance in the target encoding values. ","metadata":{}},{"cell_type":"code","source":"import category_encoders as ce\n\nte = ce.TargetEncoder(cols=cat_cols)\ndf = te.fit_transform(df, df['price'])\n","metadata":{"execution":{"iopub.status.busy":"2023-02-25T15:18:02.216877Z","iopub.execute_input":"2023-02-25T15:18:02.217292Z","iopub.status.idle":"2023-02-25T15:18:05.000956Z","shell.execute_reply.started":"2023-02-25T15:18:02.217254Z","shell.execute_reply":"2023-02-25T15:18:04.999797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2023-02-25T15:18:07.743884Z","iopub.execute_input":"2023-02-25T15:18:07.744638Z","iopub.status.idle":"2023-02-25T15:18:07.767671Z","shell.execute_reply.started":"2023-02-25T15:18:07.744597Z","shell.execute_reply":"2023-02-25T15:18:07.766588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Identifying numerical columns:","metadata":{}},{"cell_type":"code","source":"numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\nprint(numeric_cols)","metadata":{"execution":{"iopub.status.busy":"2023-02-25T15:18:12.21504Z","iopub.execute_input":"2023-02-25T15:18:12.215748Z","iopub.status.idle":"2023-02-25T15:18:12.238095Z","shell.execute_reply.started":"2023-02-25T15:18:12.215709Z","shell.execute_reply":"2023-02-25T15:18:12.237098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thus, we see that now all the columns have numerical data instead of categorical","metadata":{}},{"cell_type":"markdown","source":"## Checking for missing values:","metadata":{}},{"cell_type":"code","source":"features_with_na = [col for col in df.columns if df[col].isna().sum() > 0]\n\nmissing_values_df = pd.DataFrame(df[features_with_na].isnull().mean().sort_values(ascending=False), columns=[\"percentage\"])\nmissing_values_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-02-25T15:18:13.498865Z","iopub.execute_input":"2023-02-25T15:18:13.499528Z","iopub.status.idle":"2023-02-25T15:18:13.522527Z","shell.execute_reply.started":"2023-02-25T15:18:13.499492Z","shell.execute_reply":"2023-02-25T15:18:13.521531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### This dataset has no missing values.","metadata":{}},{"cell_type":"markdown","source":"## Scaling Data:","metadata":{}},{"cell_type":"markdown","source":"* Scaling is a preprocessing step in machine learning that aims to standardize the range or scale of the input features. The goal of scaling is to ensure that each feature has a similar scale or range, which can help some machine learning models to converge faster and improve their performance. \n\n* The choice of scaling method depends on the distribution and range of the input features, as well as the specific machine learning model being used. In general, it is a good practice to scale the data before training a machine learning model, unless the model is known to be insensitive to the scale of the input features.\n\n* One commonly used scaling method is MinMaxScaler, which scales the data to a fixed range of values between 0 and 1. It works by subtracting the minimum value of each feature and then dividing by the range (i.e., the difference between the maximum and minimum values). \n\n* The advantage of MinMaxScaler is that it preserves the shape of the original distribution and does not change the relative position of the data points. It is also relatively simple to use and understand. However, MinMaxScaler may not work well if the distribution of the data is highly skewed or has outliers, as it can magnify the effects of these outliers.","metadata":{}},{"cell_type":"code","source":"# Min Max Scaler: transformation of data\n\nnames = df.columns\nindexes = df.index\nsc = MinMaxScaler((0, 1)) #between 0 and 1 range\ndf = sc.fit_transform(df)\ndata_scaled = pd.DataFrame(df, columns=names, index=indexes)\ndata_scaled.head()","metadata":{"execution":{"iopub.status.busy":"2023-02-25T15:18:16.492685Z","iopub.execute_input":"2023-02-25T15:18:16.493591Z","iopub.status.idle":"2023-02-25T15:18:16.550399Z","shell.execute_reply.started":"2023-02-25T15:18:16.493556Z","shell.execute_reply":"2023-02-25T15:18:16.548932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Setting our target variables: ","metadata":{}},{"cell_type":"code","source":"# Set 'price' as the target variable\ny = data_scaled['price']\n\n# Extract the input features\nX_data = data_scaled.drop(['price'], axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2023-02-25T15:18:19.051098Z","iopub.execute_input":"2023-02-25T15:18:19.052098Z","iopub.status.idle":"2023-02-25T15:18:19.062628Z","shell.execute_reply.started":"2023-02-25T15:18:19.052064Z","shell.execute_reply":"2023-02-25T15:18:19.061577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature engineering:","metadata":{}},{"cell_type":"markdown","source":"Extracting the best/ most relevant features through two ways: Pearsons Correlation and kBest Features.\n\n#### 1. Pearsons correlation: \n\n* Pearson's correlation can be used as a feature engineering technique to identify and select the most relevant features for a machine learning model. By calculating the Pearson's correlation coefficient between each feature and the target variable, we can measure the linear relationship between the feature and the target and determine which features are most predictive of the target variable.\n","metadata":{}},{"cell_type":"code","source":"#Using Pearson Correlation\nplt.figure(figsize=(25,10))\ncor = data_scaled.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-25T15:21:35.173623Z","iopub.execute_input":"2023-02-25T15:21:35.174263Z","iopub.status.idle":"2023-02-25T15:21:36.166243Z","shell.execute_reply.started":"2023-02-25T15:21:35.174194Z","shell.execute_reply":"2023-02-25T15:21:36.165147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Correlation with target variable price\ncor_target = abs(cor[\"price\"])\n\nrelevant_features = cor_target\nrelevant_features \n","metadata":{"execution":{"iopub.status.busy":"2023-02-25T15:21:40.142931Z","iopub.execute_input":"2023-02-25T15:21:40.14365Z","iopub.status.idle":"2023-02-25T15:21:40.15348Z","shell.execute_reply.started":"2023-02-25T15:21:40.143613Z","shell.execute_reply":"2023-02-25T15:21:40.152246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"According to Pearsons correlation, our top 4 relevant features are: class, airline, stops and duration.","metadata":{}},{"cell_type":"markdown","source":"#### 2. Kbest Selection:\n\n* KBest feature selection is a technique in feature engineering that aims to select the k most important features from a dataset based on some statistical metric. The idea behind this technique is to reduce the dimensionality of the dataset by selecting only the most informative features, which can improve the performance of some machine learning models and reduce overfitting.\n\n* KBest feature selection works by ranking the features according to a statistical metric, such as the chi-squared test, mutual information, or f-score, and selecting the top k features with the highest scores. The specific metric used depends on the type of data and the problem at hand.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest, f_classif\n\nselector = SelectKBest(f_classif, k=4)\nX_important = selector.fit_transform(X_data, y)\n\n# Get a boolean mask of the selected features\nmask = selector.get_support()\n\n# Create a list of the selected feature names\nimportant_feature_names = X_data.columns[mask]\n\nprint(important_feature_names)","metadata":{"execution":{"iopub.status.busy":"2023-02-25T17:20:00.808059Z","iopub.execute_input":"2023-02-25T17:20:00.80856Z","iopub.status.idle":"2023-02-25T17:20:10.102991Z","shell.execute_reply.started":"2023-02-25T17:20:00.808502Z","shell.execute_reply":"2023-02-25T17:20:10.101052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"According to Kbest Features, our most important features are 'airline', 'source_city', 'destination_city', and 'class'.","metadata":{}},{"cell_type":"markdown","source":"#### Why are kBest Features and pearsons correlation giving different best features?:\n\n* The reason why KBest and Pearson's correlation coefficient can give different sets of selected features is that they are based on different assumptions and criteria. KBest feature selection evaluates the relevance of each feature based on a statistical metric, while Pearson's correlation coefficient measures the linear relationship between each feature and the target variable. Therefore, KBest feature selection may select features that are not highly correlated with the target variable but are still informative for the model, while Pearson's correlation coefficient may miss important nonlinear or non-monotonic relationships.\n\n* In practice, it is often a good idea to use multiple feature selection techniques and evaluate their performance on a validation set to choose the best set of features for the machine learning model. This can help to ensure that the selected features are relevant, informative, and not redundant.\n","metadata":{}},{"cell_type":"markdown","source":"#### So what features to select for this dataset?\n\n* Since most of the features are included by either pearsons correlation or kbest feature extraction, we will not eliminate any features and run the models on all our features.\n\n* We could also eliminate all features except 'class' and 'airline' since both the feature extraction techniques yielded these 2 as the best features","metadata":{}},{"cell_type":"markdown","source":"# Building, running and evaluating our models:","metadata":{}},{"cell_type":"markdown","source":"## What evaluation metrics are being used?\n\n#### 1. score:\n\n* The score method provides a convenient way to quickly evaluate the performance of a trained model on a test dataset, without having to manually compute the evaluation metric. \n\n* However, it is important to keep in mind that the choice of evaluation metric can have a significant impact on the performance of the model and the conclusions that can be drawn from the results. Therefore, it is often a good idea to use multiple evaluation metrics and perform cross-validation to ensure that the model is robust and generalizes well to new data.\n\n* For classification problems, model.score might return the accuracy, precision, recall, or F1 score, depending on the specific classification algorithm and the choice of evaluation metric. For regression problems, model.score might return the R-squared value, the mean absolute error, or the mean squared error, among others.\n\n#### 2. Mean squared error:\n\n* The MSE metric measures the average squared deviation of the predicted values from the actual values. It is a non-negative value where a value of zero indicates a perfect match between the predicted and actual values. A larger MSE value indicates a higher degree of error between the predicted and actual values. The MSE metric is sensitive to outliers, meaning that a few large errors can significantly increase the overall MSE value.\n\n* MSE is commonly used to evaluate the performance of regression models and can be used to compare the performance of different regression algorithms or to tune hyperparameters of a regression model.\n\n#### 3. R-squared :\n\n* R-squared (R²) is a statistical measure that tells you how well the regression model fits the data. It measures the proportion of the variance in the dependent variable (the variable you are trying to predict) that can be explained by the independent variables (the variables you are using to make the prediction).\n\n* The R-squared score ranges from 0 to 1, with a higher score indicating a better fit of the model to the data. A score of 1 means that the model explains all the variation in the dependent variable, while a score of 0 means that the model does not explain any variation.\n\n* R-squared is useful because it provides a simple way to compare the performance of different regression models. However, it only tells you how well the model fits the data overall and does not provide information about the accuracy of individual predictions. So, it is often used along with other evaluation metrics, such as mean squared error, to get a more complete understanding of the model's performance.\n\n","metadata":{}},{"cell_type":"markdown","source":"## Split the data into training and testing sets:","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_data, y, test_size=0.2, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2023-02-25T17:39:55.391993Z","iopub.execute_input":"2023-02-25T17:39:55.39239Z","iopub.status.idle":"2023-02-25T17:39:55.433397Z","shell.execute_reply.started":"2023-02-25T17:39:55.392355Z","shell.execute_reply":"2023-02-25T17:39:55.432353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Linear Regression: 90.2% accuracy","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n\nlr = LinearRegression()\n\n# Fit the model to the training data\nlr.fit(X_train, y_train)\n\n#make predictions\ny_pred = lr.predict(X_test)\n\n# Evaluate the model on the testing data\nscore = lr.score(X_test, y_test)\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Accuracy of model :\", score)\nprint(\"Mean squared error:\", mse)\nprint(\"R-squared:\", r2)","metadata":{"execution":{"iopub.status.busy":"2023-02-25T16:09:37.737906Z","iopub.execute_input":"2023-02-25T16:09:37.738619Z","iopub.status.idle":"2023-02-25T16:09:37.863477Z","shell.execute_reply.started":"2023-02-25T16:09:37.738582Z","shell.execute_reply":"2023-02-25T16:09:37.862043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Decision Tree: 94.3% accuracy","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\ndt = DecisionTreeRegressor(max_depth=5, min_samples_split=10)\n\ndt.fit(X_train, y_train)\n\ny_pred = dt.predict(X_test)\n\ndt_score = dt.score(X_test, y_test)\ndt_mse = mean_squared_error(y_test, y_pred)\ndt_r2 = r2_score(y_test, y_pred)\n\nprint(\"Accuracy of model :\", dt_score)\nprint(\"Mean squared error:\", dt_mse)\nprint(\"R-squared:\", dt_r2)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-02-25T15:56:29.644057Z","iopub.execute_input":"2023-02-25T15:56:29.645121Z","iopub.status.idle":"2023-02-25T15:56:29.892055Z","shell.execute_reply.started":"2023-02-25T15:56:29.64508Z","shell.execute_reply":"2023-02-25T15:56:29.890888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest: 96.5% accuracy - our best performing model","metadata":{"execution":{"iopub.status.busy":"2023-02-25T16:00:19.40871Z","iopub.execute_input":"2023-02-25T16:00:19.409314Z","iopub.status.idle":"2023-02-25T16:00:19.43518Z","shell.execute_reply.started":"2023-02-25T16:00:19.409277Z","shell.execute_reply":"2023-02-25T16:00:19.434105Z"}}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\n\nrf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n\nrf.fit(X_train, y_train)\n\ny_pred = rf.predict(X_test)\n\nrf_score = rf.score(X_test, y_test)\nrf_mse = mean_squared_error(y_test, y_pred)\nrf_r2 = r2_score(y_test, y_pred)\n\nprint(\"Accuracy of model :\", rf_score)\nprint(\"Mean squared error:\", rf_mse)\nprint(\"R-squared:\", rf_r2)","metadata":{"execution":{"iopub.status.busy":"2023-02-25T16:06:46.9827Z","iopub.execute_input":"2023-02-25T16:06:46.983648Z","iopub.status.idle":"2023-02-25T16:07:16.82573Z","shell.execute_reply.started":"2023-02-25T16:06:46.983581Z","shell.execute_reply":"2023-02-25T16:07:16.82453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGBoost: 95.4% accuracy","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\n\nXGB = xgb.XGBRegressor(objective ='reg:squarederror', n_estimators = 10, seed = 42)\n\nXGB.fit(X_train, y_train)\n\ny_pred = XGB.predict(X_test)\n\nXGB_score = XGB.score(X_test, y_test)\nXGB_mse = mean_squared_error(y_test, y_pred)\nXGB_r2 = r2_score(y_test, y_pred)\n\nprint(\"Accuracy of model :\", XGB_score)\nprint(\"Mean squared error:\", XGB_mse)\nprint(\"R-squared:\", XGB_r2)\n","metadata":{"execution":{"iopub.status.busy":"2023-02-25T16:11:45.762903Z","iopub.execute_input":"2023-02-25T16:11:45.763316Z","iopub.status.idle":"2023-02-25T16:11:51.617792Z","shell.execute_reply.started":"2023-02-25T16:11:45.763279Z","shell.execute_reply":"2023-02-25T16:11:51.616723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## AdaBoost: 93.4% accuracy","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostRegressor\n\nada = AdaBoostRegressor(n_estimators=50, learning_rate=0.1, random_state=42)\n\nada.fit(X_train, y_train)\n\ny_pred = ada.predict(X_test)\n\nada_score = ada.score(X_test, y_test)\nada_mse = mean_squared_error(y_test, y_pred)\nada_r2 = r2_score(y_test, y_pred)\n\nprint(\"Accuracy of model :\", ada_score)\nprint(\"Mean squared error:\", ada_mse)\nprint(\"R-squared:\", ada_r2)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-02-25T16:14:53.005742Z","iopub.execute_input":"2023-02-25T16:14:53.006745Z","iopub.status.idle":"2023-02-25T16:15:05.70649Z","shell.execute_reply.started":"2023-02-25T16:14:53.006704Z","shell.execute_reply":"2023-02-25T16:15:05.705356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Gradient Boost: 95.2% accuracy","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\n\ngb = GradientBoostingRegressor(loss='ls', learning_rate=0.1, n_estimators=100, max_depth=3)\n\ngb.fit(X_train, y_train)\n\ny_pred = gb.predict(X_test)\n\ngb_score = gb.score(X_test, y_test)\ngb_mse = mean_squared_error(y_test, y_pred)\ngb_r2 = r2_score(y_test, y_pred)\n\nprint(\"Accuracy of model :\", gb_score)\nprint(\"Mean squared error:\", gb_mse)\nprint(\"R-squared:\", gb_r2)","metadata":{"execution":{"iopub.status.busy":"2023-02-25T16:18:58.795125Z","iopub.execute_input":"2023-02-25T16:18:58.796195Z","iopub.status.idle":"2023-02-25T16:19:14.228639Z","shell.execute_reply.started":"2023-02-25T16:18:58.796156Z","shell.execute_reply":"2023-02-25T16:19:14.227488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Note that we could improve the performances of all these models by performing hyperparameter tuning.\n\n#### However, since every model gives a performance accuracy of over 90%, I have chosen to skip it. ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}